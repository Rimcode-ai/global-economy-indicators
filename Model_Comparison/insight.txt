# Key Metrics for Imbalanced Datasets:
Precision: Indicates how many predicted positives are actual positives.
Recall: Measures the ability to correctly identify actual positives.
F1-Score: Balances precision and recall, useful when both are important.

Comparison of Models:
Model                 Precision (1)   Recall (1)   F1-Score (1)   Notes for Imbalanced Datasets
Logistic Regression   0.7514          0.9994       0.8578         Excellent recall ensures most positives are identified.
SVM                   0.7511          0.9994       0.8576         Similar to Logistic Regression in performance.
Decision Tree         0.7803          0.7714       0.7758         Balanced but lower recall may miss positives.
Random Forest         0.7682          0.9234       0.8387         Good balance but lower than Logistic Regression/SVM in recall.
AdaBoost              0.7541          0.9867       0.8549         Strong recall and F1-Score.
Gradient Boosting     0.7576          0.9880       0.8576         Excellent balance of precision, recall, and F1-Score.
XGBoost               0.7728          0.9348       0.8461         Good balance; robust to class imbalance.
Naive Bayes           0.7459          0.0576       0.1070         Performs poorly; unsuitable for imbalanced datasets.

Best Models for Imbalanced Datasets:
Gradient Boosting:
- Excellent F1-Score (0.8576), high recall (0.9880), and decent precision (0.7576).
- Balances both precision and recall effectively.

Logistic Regression:
- Near-perfect recall (0.9994) ensures all positives are identified.
- F1-Score (0.8578) and precision (0.7514) are slightly lower than Gradient Boosting.

SVM:
- Performs almost identically to Logistic Regression, with slightly lower precision.

Random Forest:
- A good choice if slightly lower recall (0.9234) is acceptable for better precision (0.7682).

AdaBoost:
- Strong recall (0.9867) and F1-Score (0.8549), slightly below Gradient Boosting.

Models to Avoid for Imbalanced Datasets:
Naive Bayes:
- Very poor recall (0.0576) and F1-Score (0.1070) indicate its unsuitability for imbalanced datasets.

Recommendations:
- For imbalanced datasets where recall is critical, Logistic Regression, Gradient Boosting, or SVM should be preferred.
- If interpretability is important, Logistic Regression is a great choice.
- For high-dimensional datasets or complex relationships, Gradient Boosting or XGBoost may perform better.
"""
